{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifiers based on features extracted from matlab\n",
    "\n",
    "In this notebook we use the different arff files obtained from matlab. We will use this features to obtain classifiers and test them in a cross validation process.\n",
    "\n",
    "## A bit of set up\n",
    "\n",
    "We need numpy and pandas for data. Pickle and gzip for read the extracted features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set up Python environment: numpy for numerical routines\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# for store the results\n",
    "import pickle\n",
    "import gzip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example we only use the default linear SVM classifier from libsvm and the Gaussian NB from sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/frubio/Mat_Docs/libsvm-3.20/python/')\n",
    "\n",
    "from svmutil import *\n",
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sys.path.append('/home/frubio/PycharmProjects/mlframework/mlframework/')\n",
    "from dataset.normal_dataset import normal_dataset\n",
    "import utilsData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature reading\n",
    "\n",
    "In this example only one package is read, but each ones have a size of 80Mb approximately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = pickle.load(gzip.open('/home/frubio/python/EMtest/info.pklz','rb',2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "main_path='/home/frubio/EM_descriptors/'\n",
    "total_files=['PHOG/','CHIST.arff', 'GHIST.arff', 'GIST_ori8_block4.arff', 'Centrist.arff']\n",
    "phog_files=['2_bins360_levels0_angle360.arff', '3_bins300_levels0_angle360.arff', '4_bins200_levels0_angle360.arff',\n",
    "            '5_bins100_levels0_angle360.arff', '6_bins50_levels0_angle360.arff', '7_bins20_levels0_angle360.arff',\n",
    "            '8_bins100_levels1_angle360.arff', '9_bins50_levels1_angle360.arff', '10_bins20_levels1_angle360_redux.arff',\n",
    "            '11_bins50_levels2_angle360.arff', '12_bins20_levels2_angle360.arff']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if sys.argv[1] == '0':\n",
    "    features = utilsData.readARFF(main_path+'AVA/'+total_files[int(sys.argv[1])]+phog_files[int(sys.argv[2])])\n",
    "else:\n",
    "    features = utilsData.readARFF(main_path+'AVA/'+total_files[int(sys.argv[1])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features = utilsData.readARFF(main_path+'AVA/PHOG/10_bins20_levels1_angle360_redux.arff')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "features=pd.DataFrame(features['data'],columns=features['vars'])\n",
    "features=features.rename(columns=(lambda x: 'var'+str(int(x[3:])-1)))\n",
    "features=features.rename(columns={'var0':'id'})\n",
    "\n",
    "num_features = len(features.columns)-1\n",
    "data=pd.merge(data, features, on='id', how='right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_images = len(data)\n",
    "\n",
    "votesList=np.array(data.iloc[:,2:12])\n",
    "auxTotal=np.sum(votesList,axis=1)\n",
    "auxMeanVector=np.sum(votesList*range(1,11),axis=1)/auxTotal.astype(np.float)\n",
    "auxWeight=np.array(auxMeanVector >= 5, dtype=np.int)\n",
    "\n",
    "data.loc[:,'VotesMean'] = pd.Series(auxMeanVector, index=data.index)\n",
    "data.loc[:,'Weight'] = pd.Series(auxWeight, index=data.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data.loc[:,'id'] = data['id'].apply(str)\n",
    "classes = np.array(data.sort_values(['id']).loc[:,'Weight'])\n",
    "features = np.array(data.sort_values(['id']).iloc[:,37:num_features+37])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to use the same cases than in DeCAFF we have to separate the features and the classes in batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batches = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classes = classes[:len(classes)-(len(classes) % batches)]\n",
    "classes = classes.reshape((batches,-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features = features[:len(features)-(len(features) % batches)]\n",
    "features = features.reshape((batches,-1,num_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross validation\n",
    "\n",
    "In this case, we prepare vectors with the batches of each fold in order to test them in galgo and store the results.\n",
    "\n",
    "* First, we split the batches in 5 folds:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(1000)\n",
    "num_folds = 5\n",
    "folds = np.random.choice(range(0,batches),replace=False,size=(num_folds,batches/num_folds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def balance_class(features, classes):\n",
    "    classes_uniques = np.unique(classes)\n",
    "    min_class = np.array([0,float('Inf')])\n",
    "    for i in classes_uniques:\n",
    "        aux_value = np.sum(classes == i)\n",
    "        if aux_value < min_class[1]:\n",
    "            min_class = np.array([i,aux_value])\n",
    "            \n",
    "    final_indexes = np.where(classes == min_class[0])[0]\n",
    "    for i in classes_uniques:\n",
    "        if i != min_class[0]:\n",
    "            aux_indexes = np.where(classes == i)[0]\n",
    "            #print np.random.choice(aux_indexes,replace=False,size=min_class[1])\n",
    "            final_indexes = np.concatenate((final_indexes,np.random.choice(aux_indexes,replace=False,size=min_class[1])))\n",
    "            \n",
    "    final_indexes = np.sort(final_indexes)\n",
    "    \n",
    "    return (features[final_indexes],classes[final_indexes])\n",
    "                                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(0, num_folds):\n",
    "    \n",
    "    # Prepare train\n",
    "    train_indices = np.delete(folds,i,axis=0).reshape(-1)\n",
    "    \n",
    "    train_features = features[train_indices].reshape((-1,num_features))\n",
    "    train_classes = classes[train_indices].reshape((-1))\n",
    "    \n",
    "    train_features,train_classes = balance_class(train_features,train_classes)\n",
    "    \n",
    "    # Fit models\n",
    "    svm_clf = (prob, param)\n",
    "\n",
    "    gnb_clf = GaussianNB()\n",
    "    gnb_clf.fit(train_features, train_classes)\n",
    "    \n",
    "    # Prepare test\n",
    "    test_indices = folds[i]\n",
    "    \n",
    "    test_features = features[test_indices].reshape((-1,num_features))\n",
    "    test_classes = classes[test_indices].reshape((-1))\n",
    "    \n",
    "    # Evaluate model\n",
    "    _, p_acc, _ = svm_predict(test_classes.tolist(), test_features.tolist(), svm_clf)\n",
    "    if sys.argv[1] == '0':\n",
    "        pickle.dump(p_acc[0]/100, gzip.open( \"results/SVM_balanced_Descriptor%d_Case%d_fold%d.pklz\" % (int(sys.argv[1]),int(sys.argv[2]),i), \"wb\" ), 2)\n",
    "    else:\n",
    "        pickle.dump(p_acc[0]/100, gzip.open( \"results/SVM_balanced_Descriptor%d_fold%d.pklz\" % (int(sys.argv[1]),i), \"wb\" ), 2)\n",
    "    \n",
    "    predictions = gnb_clf.predict(test_features)\n",
    "    results = np.sum(predictions == test_classes)/float(len(predictions))\n",
    "    if sys.argv[1] == '0':\n",
    "        pickle.dump(results, gzip.open( \"results/GNB_balanced_Descriptor%d_Case%d_fold%d.pklz\" % (int(sys.argv[1]),int(sys.argv[2]),i), \"wb\" ), 2)\n",
    "    else:\n",
    "        pickle.dump(results, gzip.open( \"results/GNB_balanced_Descriptor%d_fold%d.pklz\" % (int(sys.argv[1]),i), \"wb\" ), 2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
