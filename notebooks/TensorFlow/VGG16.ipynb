{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from scipy.misc import imread, imresize\n",
    "from imagenet_classes import class_names\n",
    "from datagenerator import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Path to the textfiles for the trainings and validation set\n",
    "train_file = '../../models/train_partition_finetuning_standard_AVA_balanced.txt'\n",
    "test_file = '../../models/test_partition_finetuning_standard_AVA_balanced.txt'\n",
    "train_generator = ImageDataGenerator(train_file, shuffle = False, scale_size=(224, 224))\n",
    "test_generator = ImageDataGenerator(test_file, shuffle = False, scale_size=(224, 224)) \n",
    "\n",
    "# Learning params\n",
    "lr = 0.001\n",
    "num_epochs = 20\n",
    "batch_size = 128\n",
    "\n",
    "# Network params\n",
    "fc_nodes = 250\n",
    "num_classes = 2\n",
    "dropout_rate = 0.5\n",
    "display_step = 10\n",
    "checkpoint_path = '../../models/AesNet_VGG16_fc{}'.format(fc_nodes)\n",
    "\n",
    "# Get the number of training/validation steps per epoch\n",
    "train_batches_per_epoch = np.floor(train_generator.data_size / batch_size).astype(np.int16)\n",
    "test_batches_per_epoch = np.floor(test_generator.data_size / batch_size).astype(np.int16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "parameters = []\n",
    "g = tf.Graph()\n",
    "\n",
    "with g.as_default():\n",
    "    \n",
    "    imgs = tf.placeholder(tf.float32, [None, 224, 224, 3])\n",
    "    y = tf.placeholder(tf.float32, shape=[None, num_classes])\n",
    "    \n",
    "    keep_prob = tf.placeholder(tf.float32)\n",
    "    learning_rate = tf.placeholder(tf.float32)\n",
    "    \n",
    "    # zero-mean input\n",
    "    #with tf.name_scope('preprocess') as scope:\n",
    "    #    mean = tf.constant([123.68, 116.779, 103.939], dtype=tf.float32, shape=[1, 1, 1, 3], name='img_mean')\n",
    "    #    images = imgs-mean\n",
    "\n",
    "    # conv1_1\n",
    "    with tf.name_scope('conv1_1') as scope:\n",
    "        weights = tf.Variable(tf.truncated_normal([3, 3, 3, 64], dtype=tf.float32,stddev=1e-1),trainable=False, name='weights')\n",
    "        biases = tf.Variable(tf.constant(0.0, shape=[64], dtype=tf.float32),trainable=False, name='biases')\n",
    "        \n",
    "        conv1_1 = tf.nn.relu(tf.nn.bias_add(tf.nn.conv2d(imgs, weights, [1, 1, 1, 1], padding='SAME'),biases), name=scope)\n",
    "        \n",
    "        parameters += [weights, biases]\n",
    "        \n",
    "    # conv1_2\n",
    "    with tf.name_scope('conv1_2') as scope:\n",
    "        weights = tf.Variable(tf.truncated_normal([3, 3, 64, 64], dtype=tf.float32, stddev=1e-1),trainable=False, name='weights')\n",
    "        biases = tf.Variable(tf.constant(0.0, shape=[64], dtype=tf.float32), trainable=False, name='biases')\n",
    "        \n",
    "        conv1_2 = tf.nn.relu(tf.nn.bias_add(tf.nn.conv2d(conv1_1, weights, [1, 1, 1, 1], padding='SAME'),biases), name=scope)\n",
    "        \n",
    "        parameters += [weights, biases]\n",
    "\n",
    "    # pool1\n",
    "    pool1 = tf.nn.max_pool(conv1_2, ksize=[1, 2, 2, 1],strides=[1, 2, 2, 1],padding='SAME',name='pool1')\n",
    "\n",
    "    # conv2_1\n",
    "    with tf.name_scope('conv2_1') as scope:\n",
    "        weights = tf.Variable(tf.truncated_normal([3, 3, 64, 128], dtype=tf.float32,stddev=1e-1),trainable=False, name='weights')\n",
    "        biases = tf.Variable(tf.constant(0.0, shape=[128], dtype=tf.float32),trainable=False, name='biases')\n",
    "        \n",
    "        conv2_1 = tf.nn.relu(tf.nn.bias_add(tf.nn.conv2d(pool1, weights, [1, 1, 1, 1], padding='SAME'),biases), name=scope)\n",
    "        \n",
    "        parameters += [weights, biases]\n",
    "        \n",
    "    # conv2_2\n",
    "    with tf.name_scope('conv2_2') as scope:\n",
    "        weights = tf.Variable(tf.truncated_normal([3, 3, 128, 128], dtype=tf.float32,stddev=1e-1),trainable=False, name='weights')\n",
    "        biases = tf.Variable(tf.constant(0.0, shape=[128], dtype=tf.float32),trainable=False, name='biases')\n",
    "        \n",
    "        conv2_2 = tf.nn.relu(tf.nn.bias_add(tf.nn.conv2d(conv2_1, weights, [1, 1, 1, 1], padding='SAME'),biases), name=scope)\n",
    "        \n",
    "        parameters += [weights, biases]\n",
    "\n",
    "    # pool2\n",
    "    pool2 = tf.nn.max_pool(conv2_2,ksize=[1, 2, 2, 1],strides=[1, 2, 2, 1],padding='SAME',name='pool2')\n",
    "\n",
    "    # conv3_1\n",
    "    with tf.name_scope('conv3_1') as scope:\n",
    "        weights = tf.Variable(tf.truncated_normal([3, 3, 128, 256], dtype=tf.float32,stddev=1e-1),trainable=False, name='weights')\n",
    "        biases = tf.Variable(tf.constant(0.0, shape=[256], dtype=tf.float32),trainable=False, name='biases')\n",
    "        \n",
    "        conv3_1 = tf.nn.relu(tf.nn.bias_add(tf.nn.conv2d(pool2, weights, [1, 1, 1, 1], padding='SAME'),biases), name=scope)\n",
    "        \n",
    "        parameters += [weights, biases]\n",
    "\n",
    "    # conv3_2\n",
    "    with tf.name_scope('conv3_2') as scope:\n",
    "        weights = tf.Variable(tf.truncated_normal([3, 3, 256, 256], dtype=tf.float32,stddev=1e-1),trainable=False, name='weights')\n",
    "        biases = tf.Variable(tf.constant(0.0, shape=[256], dtype=tf.float32),trainable=False, name='biases')\n",
    "        \n",
    "        conv3_2 = tf.nn.relu(tf.nn.bias_add(tf.nn.conv2d(conv3_1, weights, [1, 1, 1, 1], padding='SAME'),biases), name=scope)\n",
    "        \n",
    "        parameters += [weights, biases]\n",
    "\n",
    "    # conv3_3\n",
    "    with tf.name_scope('conv3_3') as scope:\n",
    "        weights = tf.Variable(tf.truncated_normal([3, 3, 256, 256], dtype=tf.float32,stddev=1e-1),trainable=False, name='weights')\n",
    "        biases = tf.Variable(tf.constant(0.0, shape=[256], dtype=tf.float32),trainable=False, name='biases')\n",
    "        \n",
    "        conv3_3 = tf.nn.relu(tf.nn.bias_add(tf.nn.conv2d(conv3_2, weights, [1, 1, 1, 1], padding='SAME'),biases), name=scope)\n",
    "        \n",
    "        parameters += [weights, biases]\n",
    "\n",
    "    # pool3\n",
    "    pool3 = tf.nn.max_pool(conv3_3,ksize=[1, 2, 2, 1],strides=[1, 2, 2, 1],padding='SAME',name='pool3')\n",
    "\n",
    "    # conv4_1\n",
    "    with tf.name_scope('conv4_1') as scope:\n",
    "        weights = tf.Variable(tf.truncated_normal([3, 3, 256, 512], dtype=tf.float32,stddev=1e-1),trainable=False, name='weights')\n",
    "        biases = tf.Variable(tf.constant(0.0, shape=[512], dtype=tf.float32),trainable=False, name='biases')\n",
    "        \n",
    "        conv4_1 = tf.nn.relu(tf.nn.bias_add(tf.nn.conv2d(pool3, weights, [1, 1, 1, 1], padding='SAME'),biases), name=scope)\n",
    "        \n",
    "        parameters += [weights, biases]\n",
    "\n",
    "    # conv4_2\n",
    "    with tf.name_scope('conv4_2') as scope:\n",
    "        weights = tf.Variable(tf.truncated_normal([3, 3, 512, 512], dtype=tf.float32,stddev=1e-1),trainable=False, name='weights')\n",
    "        biases = tf.Variable(tf.constant(0.0, shape=[512], dtype=tf.float32),trainable=False, name='biases')\n",
    "        \n",
    "        conv4_2 = tf.nn.relu(tf.nn.bias_add(tf.nn.conv2d(conv4_1, weights, [1, 1, 1, 1], padding='SAME'),biases), name=scope)\n",
    "        \n",
    "        parameters += [weights, biases]\n",
    "\n",
    "    # conv4_3\n",
    "    with tf.name_scope('conv4_3') as scope:\n",
    "        weights = tf.Variable(tf.truncated_normal([3, 3, 512, 512], dtype=tf.float32,stddev=1e-1),trainable=False, name='weights')\n",
    "        biases = tf.Variable(tf.constant(0.0, shape=[512], dtype=tf.float32),trainable=False, name='biases')\n",
    "        \n",
    "        conv4_3 = tf.nn.relu(tf.nn.bias_add(tf.nn.conv2d(conv4_2, weights, [1, 1, 1, 1], padding='SAME'),biases), name=scope)\n",
    "        \n",
    "        parameters += [weights, biases]\n",
    "\n",
    "    # pool4\n",
    "    pool4 = tf.nn.max_pool(conv4_3,ksize=[1, 2, 2, 1],strides=[1, 2, 2, 1],padding='SAME',name='pool4')\n",
    "\n",
    "    # conv5_1\n",
    "    with tf.name_scope('conv5_1') as scope:\n",
    "        weights = tf.Variable(tf.truncated_normal([3, 3, 512, 512], dtype=tf.float32,stddev=1e-1),trainable=False, name='weights')\n",
    "        biases = tf.Variable(tf.constant(0.0, shape=[512], dtype=tf.float32),trainable=False, name='biases')\n",
    "        \n",
    "        conv5_1 = tf.nn.relu(tf.nn.bias_add(tf.nn.conv2d(pool4, weights, [1, 1, 1, 1], padding='SAME'),biases), name=scope)\n",
    "        \n",
    "        parameters += [weights, biases]\n",
    "\n",
    "    # conv5_2\n",
    "    with tf.name_scope('conv5_2') as scope:\n",
    "        weights = tf.Variable(tf.truncated_normal([3, 3, 512, 512], dtype=tf.float32,stddev=1e-1),trainable=False, name='weights')\n",
    "        biases = tf.Variable(tf.constant(0.0, shape=[512], dtype=tf.float32),trainable=False, name='biases')\n",
    "        \n",
    "        conv5_2 = tf.nn.relu(tf.nn.bias_add(tf.nn.conv2d(conv5_1, weights, [1, 1, 1, 1], padding='SAME'),biases), name=scope)\n",
    "        \n",
    "        parameters += [weights, biases]\n",
    "\n",
    "    # conv5_3\n",
    "    with tf.name_scope('conv5_3') as scope:\n",
    "        weights = tf.Variable(tf.truncated_normal([3, 3, 512, 512], dtype=tf.float32,stddev=1e-1),trainable=False, name='weights')\n",
    "        biases = tf.Variable(tf.constant(0.0, shape=[512], dtype=tf.float32),trainable=False, name='biases')\n",
    "        \n",
    "        conv5_3 = tf.nn.relu(tf.nn.bias_add(tf.nn.conv2d(conv5_2, weights, [1, 1, 1, 1], padding='SAME'),biases), name=scope)\n",
    "        \n",
    "        parameters += [weights, biases]\n",
    "\n",
    "    # pool5\n",
    "    pool5 = tf.nn.max_pool(conv5_3,ksize=[1, 2, 2, 1],strides=[1, 2, 2, 1],padding='SAME',name='pool5')\n",
    "\n",
    "    # fc6\n",
    "    with tf.name_scope('fc6') as scope:\n",
    "        shape = int(np.prod(pool5.get_shape()[1:]))\n",
    "        weights = tf.Variable(tf.truncated_normal([shape, fc_nodes],dtype=tf.float32,stddev=1e-1), name='weights')\n",
    "        biases = tf.Variable(tf.constant(1.0, shape=[fc_nodes], dtype=tf.float32),trainable=True, name='biases')\n",
    "        \n",
    "        pool5_flat = tf.reshape(pool5, [-1, shape])\n",
    "        fc6 = tf.nn.relu(tf.nn.bias_add(tf.matmul(pool5_flat, weights), biases))\n",
    "        \n",
    "        parameters += [weights, biases]\n",
    "        \n",
    "        fc6_drop = tf.nn.dropout(fc6, keep_prob)\n",
    "\n",
    "    # fc7\n",
    "    with tf.name_scope('fc7') as scope:\n",
    "        weights = tf.Variable(tf.truncated_normal([fc_nodes, fc_nodes],dtype=tf.float32, stddev=1e-1), name='weights')\n",
    "        biases = tf.Variable(tf.constant(1.0, shape=[fc_nodes], dtype=tf.float32),trainable=True, name='biases')\n",
    "        \n",
    "        fc7 = tf.nn.relu(tf.nn.bias_add(tf.matmul(fc6_drop, weights), biases))\n",
    "        \n",
    "        parameters += [weights, biases]\n",
    "        \n",
    "        fc7_drop = tf.nn.dropout(fc7, keep_prob)\n",
    "\n",
    "    # fc8 (output)\n",
    "    with tf.name_scope('fc8') as scope:\n",
    "        weights = tf.Variable(tf.truncated_normal([fc_nodes, num_classes],dtype=tf.float32,stddev=1e-1), name='weights')\n",
    "        biases = tf.Variable(tf.constant(1.0, shape=[num_classes], dtype=tf.float32),trainable=True, name='biases')\n",
    "        \n",
    "        fc8 = tf.nn.bias_add(tf.matmul(fc7_drop, weights), biases)\n",
    "        \n",
    "        parameters += [weights, biases]\n",
    "        \n",
    "    # Op for calculating the loss\n",
    "    with tf.name_scope(\"cross_ent\"):\n",
    "        loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = fc8, labels = y))  \n",
    "\n",
    "    # Train op\n",
    "    with tf.name_scope(\"train\"):\n",
    "        # Get gradients of all trainable variables\n",
    "        var_list = tf.trainable_variables()\n",
    "        #gradients = tf.gradients(loss, var_list)\n",
    "        #gradients = list(zip(gradients, var_list))\n",
    "\n",
    "        # Create optimizer and apply gradient descent to the trainable variables\n",
    "        optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "        gradients = optimizer.compute_gradients(loss, var_list)\n",
    "        train_op = optimizer.apply_gradients(gradients)\n",
    "        \n",
    "    correct_prediction = tf.equal(tf.argmax(fc8, 1), tf.argmax(y, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "    saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "with tf.Session(graph=g) as sess:\n",
    "\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    weight_file =\"vgg16_weights.npz\"\n",
    "    weights = np.load(weight_file)\n",
    "    keys = sorted(weights.keys())\n",
    "    for i, k in enumerate(keys):\n",
    "        #print (i, parameters[i].name, k, np.shape(weights[k]))\n",
    "        if (i<=25):\n",
    "            sess.run(parameters[i].assign(weights[k]))\n",
    "            \n",
    "    for epoch in range(num_epochs):\n",
    "        if(epoch > 0 and epoch%7==0):\n",
    "            lr *= 0.1\n",
    "            \n",
    "        step = 1\n",
    "        while step < train_batches_per_epoch:\n",
    "\n",
    "            # Get a batch of images and labels\n",
    "            batch_xs, batch_ys = train_generator.next_batch(batch_size)\n",
    "\n",
    "            # And run the training op\n",
    "            sess.run(train_op, feed_dict={imgs: batch_xs, \n",
    "                                          y: batch_ys, \n",
    "                                          keep_prob: dropout_rate,\n",
    "                                          learning_rate: lr})\n",
    "            step += 1\n",
    "        \n",
    "            if(step % 10 == 0):\n",
    "                aux_accu = accuracy.eval(feed_dict={imgs: batch_xs, \n",
    "                                                        y: batch_ys, \n",
    "                                                        keep_prob: 1.,\n",
    "                                                        learning_rate: lr})\n",
    "                print(epoch+1, aux_accu)\n",
    "\n",
    "                print(\"{} Saving checkpoint of model...\".format(datetime.now()))  \n",
    "\n",
    "                #save checkpoint of the model\n",
    "                checkpoint_name = os.path.join(checkpoint_path, 'model_epoch'+str(epoch+1)+'.ckpt')\n",
    "                save_path = saver.save(sess, checkpoint_name)  \n",
    "\n",
    "                print(\"{} Model checkpoint saved at {}\".format(datetime.now(), checkpoint_name))\n",
    "            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
