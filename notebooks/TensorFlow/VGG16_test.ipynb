{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from scipy.misc import imread, imresize\n",
    "from imagenet_classes import class_names\n",
    "from datagenerator import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Path to the textfiles for the trainings and validation set\n",
    "train_file = '../../models/train_partition_finetuning_standard_AVA_balanced.txt'\n",
    "test_file = '../../models/test_partition_finetuning_standard_AVA_balanced.txt'\n",
    "train_generator = ImageDataGenerator(train_file, shuffle = False, scale_size=(224, 224))\n",
    "test_generator = ImageDataGenerator(test_file, shuffle = False, scale_size=(224, 224)) \n",
    "\n",
    "# Learning params\n",
    "lr = 0.001\n",
    "num_epochs = 20\n",
    "batch_size = 128\n",
    "\n",
    "# Network params\n",
    "fc_nodes = 1000\n",
    "num_classes = 2\n",
    "dropout_rate = 0.5\n",
    "display_step = 10\n",
    "checkpoint_path = '../../models/AesNet_VGG16_fc{}'.format(fc_nodes)\n",
    "\n",
    "# Get the number of training/validation steps per epoch\n",
    "train_batches_per_epoch = np.floor(train_generator.data_size / batch_size).astype(np.int16)\n",
    "test_batches_per_epoch = np.floor(test_generator.data_size / batch_size).astype(np.int16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "parameters = []\n",
    "g = tf.Graph()\n",
    "\n",
    "with g.as_default():\n",
    "    \n",
    "    imgs = tf.placeholder(tf.float32, [None, 224, 224, 3])\n",
    "    y = tf.placeholder(tf.float32, shape=[None, num_classes])\n",
    "    \n",
    "    keep_prob = tf.placeholder(tf.float32)\n",
    "    learning_rate = tf.placeholder(tf.float32)\n",
    "    \n",
    "    # zero-mean input\n",
    "    #with tf.name_scope('preprocess') as scope:\n",
    "    #    mean = tf.constant([123.68, 116.779, 103.939], dtype=tf.float32, shape=[1, 1, 1, 3], name='img_mean')\n",
    "    #    images = imgs-mean\n",
    "\n",
    "    # conv1_1\n",
    "    with tf.name_scope('conv1_1') as scope:\n",
    "        weights = tf.Variable(tf.truncated_normal([3, 3, 3, 64], dtype=tf.float32,stddev=1e-1),trainable=False, name='weights')\n",
    "        biases = tf.Variable(tf.constant(0.0, shape=[64], dtype=tf.float32),trainable=False, name='biases')\n",
    "        \n",
    "        conv1_1 = tf.nn.relu(tf.nn.bias_add(tf.nn.conv2d(imgs, weights, [1, 1, 1, 1], padding='SAME'),biases), name=scope)\n",
    "        \n",
    "        parameters += [weights, biases]\n",
    "        \n",
    "    # conv1_2\n",
    "    with tf.name_scope('conv1_2') as scope:\n",
    "        weights = tf.Variable(tf.truncated_normal([3, 3, 64, 64], dtype=tf.float32, stddev=1e-1),trainable=False, name='weights')\n",
    "        biases = tf.Variable(tf.constant(0.0, shape=[64], dtype=tf.float32), trainable=False, name='biases')\n",
    "        \n",
    "        conv1_2 = tf.nn.relu(tf.nn.bias_add(tf.nn.conv2d(conv1_1, weights, [1, 1, 1, 1], padding='SAME'),biases), name=scope)\n",
    "        \n",
    "        parameters += [weights, biases]\n",
    "\n",
    "    # pool1\n",
    "    pool1 = tf.nn.max_pool(conv1_2, ksize=[1, 2, 2, 1],strides=[1, 2, 2, 1],padding='SAME',name='pool1')\n",
    "\n",
    "    # conv2_1\n",
    "    with tf.name_scope('conv2_1') as scope:\n",
    "        weights = tf.Variable(tf.truncated_normal([3, 3, 64, 128], dtype=tf.float32,stddev=1e-1),trainable=False, name='weights')\n",
    "        biases = tf.Variable(tf.constant(0.0, shape=[128], dtype=tf.float32),trainable=False, name='biases')\n",
    "        \n",
    "        conv2_1 = tf.nn.relu(tf.nn.bias_add(tf.nn.conv2d(pool1, weights, [1, 1, 1, 1], padding='SAME'),biases), name=scope)\n",
    "        \n",
    "        parameters += [weights, biases]\n",
    "        \n",
    "    # conv2_2\n",
    "    with tf.name_scope('conv2_2') as scope:\n",
    "        weights = tf.Variable(tf.truncated_normal([3, 3, 128, 128], dtype=tf.float32,stddev=1e-1),trainable=False, name='weights')\n",
    "        biases = tf.Variable(tf.constant(0.0, shape=[128], dtype=tf.float32),trainable=False, name='biases')\n",
    "        \n",
    "        conv2_2 = tf.nn.relu(tf.nn.bias_add(tf.nn.conv2d(conv2_1, weights, [1, 1, 1, 1], padding='SAME'),biases), name=scope)\n",
    "        \n",
    "        parameters += [weights, biases]\n",
    "\n",
    "    # pool2\n",
    "    pool2 = tf.nn.max_pool(conv2_2,ksize=[1, 2, 2, 1],strides=[1, 2, 2, 1],padding='SAME',name='pool2')\n",
    "\n",
    "    # conv3_1\n",
    "    with tf.name_scope('conv3_1') as scope:\n",
    "        weights = tf.Variable(tf.truncated_normal([3, 3, 128, 256], dtype=tf.float32,stddev=1e-1),trainable=False, name='weights')\n",
    "        biases = tf.Variable(tf.constant(0.0, shape=[256], dtype=tf.float32),trainable=False, name='biases')\n",
    "        \n",
    "        conv3_1 = tf.nn.relu(tf.nn.bias_add(tf.nn.conv2d(pool2, weights, [1, 1, 1, 1], padding='SAME'),biases), name=scope)\n",
    "        \n",
    "        parameters += [weights, biases]\n",
    "\n",
    "    # conv3_2\n",
    "    with tf.name_scope('conv3_2') as scope:\n",
    "        weights = tf.Variable(tf.truncated_normal([3, 3, 256, 256], dtype=tf.float32,stddev=1e-1),trainable=False, name='weights')\n",
    "        biases = tf.Variable(tf.constant(0.0, shape=[256], dtype=tf.float32),trainable=False, name='biases')\n",
    "        \n",
    "        conv3_2 = tf.nn.relu(tf.nn.bias_add(tf.nn.conv2d(conv3_1, weights, [1, 1, 1, 1], padding='SAME'),biases), name=scope)\n",
    "        \n",
    "        parameters += [weights, biases]\n",
    "\n",
    "    # conv3_3\n",
    "    with tf.name_scope('conv3_3') as scope:\n",
    "        weights = tf.Variable(tf.truncated_normal([3, 3, 256, 256], dtype=tf.float32,stddev=1e-1),trainable=False, name='weights')\n",
    "        biases = tf.Variable(tf.constant(0.0, shape=[256], dtype=tf.float32),trainable=False, name='biases')\n",
    "        \n",
    "        conv3_3 = tf.nn.relu(tf.nn.bias_add(tf.nn.conv2d(conv3_2, weights, [1, 1, 1, 1], padding='SAME'),biases), name=scope)\n",
    "        \n",
    "        parameters += [weights, biases]\n",
    "\n",
    "    # pool3\n",
    "    pool3 = tf.nn.max_pool(conv3_3,ksize=[1, 2, 2, 1],strides=[1, 2, 2, 1],padding='SAME',name='pool3')\n",
    "\n",
    "    # conv4_1\n",
    "    with tf.name_scope('conv4_1') as scope:\n",
    "        weights = tf.Variable(tf.truncated_normal([3, 3, 256, 512], dtype=tf.float32,stddev=1e-1),trainable=False, name='weights')\n",
    "        biases = tf.Variable(tf.constant(0.0, shape=[512], dtype=tf.float32),trainable=False, name='biases')\n",
    "        \n",
    "        conv4_1 = tf.nn.relu(tf.nn.bias_add(tf.nn.conv2d(pool3, weights, [1, 1, 1, 1], padding='SAME'),biases), name=scope)\n",
    "        \n",
    "        parameters += [weights, biases]\n",
    "\n",
    "    # conv4_2\n",
    "    with tf.name_scope('conv4_2') as scope:\n",
    "        weights = tf.Variable(tf.truncated_normal([3, 3, 512, 512], dtype=tf.float32,stddev=1e-1),trainable=False, name='weights')\n",
    "        biases = tf.Variable(tf.constant(0.0, shape=[512], dtype=tf.float32),trainable=False, name='biases')\n",
    "        \n",
    "        conv4_2 = tf.nn.relu(tf.nn.bias_add(tf.nn.conv2d(conv4_1, weights, [1, 1, 1, 1], padding='SAME'),biases), name=scope)\n",
    "        \n",
    "        parameters += [weights, biases]\n",
    "\n",
    "    # conv4_3\n",
    "    with tf.name_scope('conv4_3') as scope:\n",
    "        weights = tf.Variable(tf.truncated_normal([3, 3, 512, 512], dtype=tf.float32,stddev=1e-1),trainable=False, name='weights')\n",
    "        biases = tf.Variable(tf.constant(0.0, shape=[512], dtype=tf.float32),trainable=False, name='biases')\n",
    "        \n",
    "        conv4_3 = tf.nn.relu(tf.nn.bias_add(tf.nn.conv2d(conv4_2, weights, [1, 1, 1, 1], padding='SAME'),biases), name=scope)\n",
    "        \n",
    "        parameters += [weights, biases]\n",
    "\n",
    "    # pool4\n",
    "    pool4 = tf.nn.max_pool(conv4_3,ksize=[1, 2, 2, 1],strides=[1, 2, 2, 1],padding='SAME',name='pool4')\n",
    "\n",
    "    # conv5_1\n",
    "    with tf.name_scope('conv5_1') as scope:\n",
    "        weights = tf.Variable(tf.truncated_normal([3, 3, 512, 512], dtype=tf.float32,stddev=1e-1),trainable=False, name='weights')\n",
    "        biases = tf.Variable(tf.constant(0.0, shape=[512], dtype=tf.float32),trainable=False, name='biases')\n",
    "        \n",
    "        conv5_1 = tf.nn.relu(tf.nn.bias_add(tf.nn.conv2d(pool4, weights, [1, 1, 1, 1], padding='SAME'),biases), name=scope)\n",
    "        \n",
    "        parameters += [weights, biases]\n",
    "\n",
    "    # conv5_2\n",
    "    with tf.name_scope('conv5_2') as scope:\n",
    "        weights = tf.Variable(tf.truncated_normal([3, 3, 512, 512], dtype=tf.float32,stddev=1e-1),trainable=False, name='weights')\n",
    "        biases = tf.Variable(tf.constant(0.0, shape=[512], dtype=tf.float32),trainable=False, name='biases')\n",
    "        \n",
    "        conv5_2 = tf.nn.relu(tf.nn.bias_add(tf.nn.conv2d(conv5_1, weights, [1, 1, 1, 1], padding='SAME'),biases), name=scope)\n",
    "        \n",
    "        parameters += [weights, biases]\n",
    "\n",
    "    # conv5_3\n",
    "    with tf.name_scope('conv5_3') as scope:\n",
    "        weights = tf.Variable(tf.truncated_normal([3, 3, 512, 512], dtype=tf.float32,stddev=1e-1),trainable=False, name='weights')\n",
    "        biases = tf.Variable(tf.constant(0.0, shape=[512], dtype=tf.float32),trainable=False, name='biases')\n",
    "        \n",
    "        conv5_3 = tf.nn.relu(tf.nn.bias_add(tf.nn.conv2d(conv5_2, weights, [1, 1, 1, 1], padding='SAME'),biases), name=scope)\n",
    "        \n",
    "        parameters += [weights, biases]\n",
    "\n",
    "    # pool5\n",
    "    pool5 = tf.nn.max_pool(conv5_3,ksize=[1, 2, 2, 1],strides=[1, 2, 2, 1],padding='SAME',name='pool5')\n",
    "\n",
    "    # fc6\n",
    "    with tf.name_scope('fc6') as scope:\n",
    "        shape = int(np.prod(pool5.get_shape()[1:]))\n",
    "        weights = tf.Variable(tf.truncated_normal([shape, fc_nodes],dtype=tf.float32,stddev=1e-1), name='weights')\n",
    "        biases = tf.Variable(tf.constant(1.0, shape=[fc_nodes], dtype=tf.float32),trainable=True, name='biases')\n",
    "        \n",
    "        pool5_flat = tf.reshape(pool5, [-1, shape])\n",
    "        fc6 = tf.nn.relu(tf.nn.bias_add(tf.matmul(pool5_flat, weights), biases))\n",
    "        \n",
    "        parameters += [weights, biases]\n",
    "        \n",
    "        fc6_drop = tf.nn.dropout(fc6, keep_prob)\n",
    "\n",
    "    # fc7\n",
    "    with tf.name_scope('fc7') as scope:\n",
    "        weights = tf.Variable(tf.truncated_normal([fc_nodes, fc_nodes],dtype=tf.float32, stddev=1e-1), name='weights')\n",
    "        biases = tf.Variable(tf.constant(1.0, shape=[fc_nodes], dtype=tf.float32),trainable=True, name='biases')\n",
    "        \n",
    "        fc7 = tf.nn.relu(tf.nn.bias_add(tf.matmul(fc6_drop, weights), biases))\n",
    "        \n",
    "        parameters += [weights, biases]\n",
    "        \n",
    "        fc7_drop = tf.nn.dropout(fc7, keep_prob)\n",
    "\n",
    "    # fc8 (output)\n",
    "    with tf.name_scope('fc8') as scope:\n",
    "        weights = tf.Variable(tf.truncated_normal([fc_nodes, num_classes],dtype=tf.float32,stddev=1e-1), name='weights')\n",
    "        biases = tf.Variable(tf.constant(1.0, shape=[num_classes], dtype=tf.float32),trainable=True, name='biases')\n",
    "        \n",
    "        fc8 = tf.nn.bias_add(tf.matmul(fc7_drop, weights), biases)\n",
    "        \n",
    "        parameters += [weights, biases]\n",
    "        \n",
    "    # Op for calculating the loss\n",
    "    with tf.name_scope(\"cross_ent\"):\n",
    "        loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = fc8, labels = y))  \n",
    "\n",
    "    # Train op\n",
    "    with tf.name_scope(\"train\"):\n",
    "        # Get gradients of all trainable variables\n",
    "        var_list = tf.trainable_variables()\n",
    "        #gradients = tf.gradients(loss, var_list)\n",
    "        #gradients = list(zip(gradients, var_list))\n",
    "\n",
    "        # Create optimizer and apply gradient descent to the trainable variables\n",
    "        optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "        gradients = optimizer.compute_gradients(loss, var_list)\n",
    "        train_op = optimizer.apply_gradients(gradients)\n",
    "        \n",
    "    correct_prediction = tf.equal(tf.argmax(fc8, 1), tf.argmax(y, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "    saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "with tf.Session(graph=g) as sess:\n",
    "\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    weight_file =\"vgg16_weights.npz\"\n",
    "    weights = np.load(weight_file)\n",
    "    keys = sorted(weights.keys())\n",
    "    for i, k in enumerate(keys):\n",
    "        #print (i, parameters[i].name, k, np.shape(weights[k]))\n",
    "        if (i<=25):\n",
    "            sess.run(parameters[i].assign(weights[k]))\n",
    "            \n",
    "    for epoch in range(num_epochs):\n",
    "        if(epoch > 0 and epoch%7==0):\n",
    "            lr *= 0.1\n",
    "            \n",
    "        step = 1\n",
    "        while step < train_batches_per_epoch:\n",
    "\n",
    "            # Get a batch of images and labels\n",
    "            batch_xs, batch_ys = train_generator.next_batch(batch_size)\n",
    "\n",
    "            # And run the training op\n",
    "            sess.run(train_op, feed_dict={imgs: batch_xs, \n",
    "                                          y: batch_ys, \n",
    "                                          keep_prob: dropout_rate,\n",
    "                                          learning_rate: lr})\n",
    "            step += 1\n",
    "        \n",
    "            if(step % 10 == 0):\n",
    "                aux_accu = accuracy.eval(feed_dict={imgs: batch_xs, \n",
    "                                                        y: batch_ys, \n",
    "                                                        keep_prob: 1.,\n",
    "                                                        learning_rate: lr})\n",
    "                print(epoch+1, aux_accu)\n",
    "\n",
    "                print(\"{} Saving checkpoint of model...\".format(datetime.now()))  \n",
    "\n",
    "                #save checkpoint of the model\n",
    "                checkpoint_name = os.path.join(checkpoint_path, 'model_epoch'+str(epoch+1)+'.ckpt')\n",
    "                save_path = saver.save(sess, checkpoint_name)  \n",
    "\n",
    "                print(\"{} Model checkpoint saved at {}\".format(datetime.now(), checkpoint_name))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "2017-12-04 09:53:08.688691 Test Accuracy = 0.2887\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with tf.Session(graph=g) as sess:\n",
    "    # Restore variables from disk.\n",
    "    saver.restore(sess, \"../../models/AesNet_VGG16_fc1000/model_epoch20.ckpt\")\n",
    "\n",
    "    test_acc = 0.\n",
    "    test_count = 0\n",
    "    for _ in range(test_batches_per_epoch):\n",
    "        batch_tx, batch_ty = test_generator.next_batch(batch_size)\n",
    "        acc = sess.run(accuracy, feed_dict={imgs: batch_tx, y: batch_ty, keep_prob: 1.,learning_rate: lr})\n",
    "        test_acc += acc\n",
    "        test_count += 1\n",
    "        print(test_count)\n",
    "    test_acc /= test_count\n",
    "    print(\"{} Test Accuracy = {:.4f}\".format(datetime.now(), test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
