{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Classifiers based on features extracted from matlab\n",
    "\n",
    "In this notebook we use the different arff files obtained from matlab. We will use this features to obtain classifiers and test them in a cross validation process.\n",
    "\n",
    "## A bit of set up\n",
    "\n",
    "We need numpy and pandas for data. Pickle and gzip for read the extracted features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# set up Python environment: numpy for numerical routines\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# for store the results\n",
    "import pickle\n",
    "import gzip\n",
    "\n",
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "In this example we only use the default linear SVM classifier from libsvm and the Gaussian NB from sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn_extensions.extreme_learning_machines.elm import GenELMClassifier\n",
    "from sklearn_extensions.extreme_learning_machines.random_layer import RBFRandomLayer, MLPRandomLayer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../pycode/')\n",
    "import utilsData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Cross validation\n",
    "\n",
    "In this case, we prepare vectors with the batches of each fold in order to test them in galgo and store the results.\n",
    "\n",
    "* First, we split the batches in 5 folds:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def check_ELM(string_base):\n",
    "\n",
    "    num_folds = 5\n",
    "    nh = 1000\n",
    "\n",
    "    # pass user defined transfer func\n",
    "    sinsq = (lambda x: np.power(np.sin(x), 2.0))\n",
    "    srhl_sinsq = MLPRandomLayer(n_hidden=nh, activation_func=sinsq)\n",
    "\n",
    "    # use internal transfer funcs\n",
    "    srhl_tanh = MLPRandomLayer(n_hidden=nh, activation_func='tanh')\n",
    "\n",
    "    classifiers = [GenELMClassifier(hidden_layer=srhl_tanh),\n",
    "                   GenELMClassifier(hidden_layer=srhl_sinsq)]\n",
    "    \n",
    "    scores = np.zeros(len(classifiers))\n",
    "\n",
    "    for i in range(0,num_folds):\n",
    "\n",
    "        features_train = utilsData.readARFF('../features/arff/file_{}_t2_to{:d}_training.arff'.format(string_base,i+1))\n",
    "        features_test = utilsData.readARFF('../features/arff/file_{}_t2_to{:d}_test.arff'.format(string_base,i+1))\n",
    "\n",
    "        features_train=pd.DataFrame(features_train['data'],columns=features_train['vars'])\n",
    "        features_train=features_train.rename(columns=(lambda x: 'var'+str(int(x[3:])-1)))\n",
    "\n",
    "        features_test=pd.DataFrame(features_test['data'],columns=features_test['vars'])\n",
    "        features_test=features_test.rename(columns=(lambda x: 'var'+str(int(x[3:])-1)))\n",
    "\n",
    "        for idx, clf in enumerate(classifiers):\n",
    "            clf.fit(features_train.iloc[:,0:-1], features_train.iloc[:,-1])\n",
    "            scores[idx] += clf.score(features_test.iloc[:,0:-1], features_test.iloc[:,-1])\n",
    "\n",
    "    return scores/num_folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "database: 0\n",
      "descriptor: 0\n",
      "descriptor_size: 50\n",
      "[ 0.89033654  0.89350474]\n",
      "\n",
      "descriptor_size: 100\n",
      "[ 0.87299816  0.87000521]\n",
      "\n",
      "descriptor_size: 200\n",
      "[ 0.84694729  0.84976241]\n",
      "\n",
      "descriptor_size: 300\n",
      "[ 0.83427212  0.83620963]\n",
      "\n",
      "descriptor: 1\n",
      "descriptor_size: 50\n",
      "[ 0.93328611  0.93117429]\n",
      "\n",
      "descriptor_size: 100\n",
      "[ 0.94789654  0.94939275]\n",
      "\n",
      "descriptor_size: 200\n",
      "[ 0.9515938   0.95027218]\n",
      "\n",
      "descriptor_size: 300\n",
      "[ 0.95361706  0.95432109]\n",
      "\n",
      "database: 2\n",
      "descriptor: 0\n",
      "descriptor_size: 50\n",
      "[ 0.84117461  0.84061062]\n",
      "\n",
      "descriptor_size: 100\n",
      "[ 0.82642798  0.82614682]\n",
      "\n",
      "descriptor_size: 200\n",
      "[ 0.79994275  0.80322918]\n",
      "\n",
      "descriptor_size: 300\n",
      "[ 0.7815341   0.78181509]\n",
      "\n",
      "descriptor: 1\n",
      "descriptor_size: 50\n",
      "[ 0.92786642  0.93049584]\n",
      "\n",
      "descriptor_size: 100\n",
      "[ 0.94336436  0.94467927]\n",
      "\n",
      "descriptor_size: 200\n",
      "[ 0.9489059   0.94740267]\n",
      "\n",
      "descriptor_size: 300\n",
      "[ 0.94909277  0.9494702 ]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "list_to_pandas = []\n",
    "for i in [0,2]:\n",
    "    print (\"database: {:d}\".format(i))\n",
    "    if i == 0: \n",
    "        robot = 1 \n",
    "    else:\n",
    "        robot = 0\n",
    "    for j in [0,1]:\n",
    "        print (\"descriptor: {:d}\".format(j))\n",
    "        for k in [50,100,200,300]:\n",
    "            print (\"descriptor_size: {:d}\".format(k))\n",
    "            results = check_ELM('da{:d}_r{:d}_de{:d}_v{:d}_ci0'.format(i,robot,j,k))\n",
    "            print (results)\n",
    "            list_to_pandas.append([i,robot,j,k,0,results[1]])\n",
    "            print ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "database: 0\n",
      "descriptor: 0\n",
      "descriptor_size: 50\n",
      "[ 0.88171132  0.87854367]\n",
      "\n",
      "descriptor_size: 100\n",
      "[ 0.86032416  0.85900436]\n",
      "\n",
      "descriptor_size: 200\n",
      "[ 0.83295181  0.82846319]\n",
      "\n",
      "descriptor_size: 300\n",
      "[ 0.80135663  0.79783442]\n",
      "\n",
      "descriptor: 1\n",
      "descriptor_size: 50\n",
      "[ 0.92580492  0.91718013]\n",
      "\n",
      "descriptor_size: 100\n",
      "[ 0.94199923  0.93777504]\n",
      "\n",
      "descriptor_size: 200\n",
      "[ 0.94516856  0.94182364]\n",
      "\n",
      "descriptor_size: 300\n",
      "[ 0.94824877  0.94094352]\n",
      "\n",
      "database: 2\n",
      "descriptor: 0\n",
      "descriptor_size: 50\n",
      "[ 0.8517886   0.85254026]\n",
      "\n",
      "descriptor_size: 100\n",
      "[ 0.83779438  0.83892242]\n",
      "\n",
      "descriptor_size: 200\n",
      "[ 0.81807068  0.82577229]\n",
      "\n",
      "descriptor_size: 300\n",
      "[ 0.80266774  0.80924179]\n",
      "\n",
      "descriptor: 1\n",
      "descriptor_size: 50\n",
      "[ 0.92739658  0.92542475]\n",
      "\n",
      "descriptor_size: 100\n",
      "[ 0.94092181  0.93669597]\n",
      "\n",
      "descriptor_size: 200\n",
      "[ 0.9420491   0.93697722]\n",
      "\n",
      "descriptor_size: 300\n",
      "[ 0.9438334  0.9365073]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in [0,2]:\n",
    "    print (\"database: {:d}\".format(i))\n",
    "    if i == 0: \n",
    "        robot = 1 \n",
    "        ci = 3\n",
    "    else:\n",
    "        robot = 0\n",
    "        ci = 1\n",
    "    for j in [0,1]:\n",
    "        print (\"descriptor: {:d}\".format(j))\n",
    "        for k in [50,100,200,300]:\n",
    "            print (\"descriptor_size: {:d}\".format(k))\n",
    "            results = check_ELM('da{:d}_r{:d}_de{:d}_v{:d}_ci{:d}'.format(i,robot,j,k,ci))\n",
    "            print (results)\n",
    "            list_to_pandas.append([i,robot,j,k,1,results[1]])\n",
    "            print ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 1, 0, 50, 0, 0.89350473717801759],\n",
       " [0, 1, 0, 100, 0, 0.87000520500920175],\n",
       " [0, 1, 0, 200, 0, 0.84976240527193081],\n",
       " [0, 1, 0, 300, 0, 0.83620963019648897],\n",
       " [0, 1, 1, 50, 0, 0.93117428725454354],\n",
       " [0, 1, 1, 100, 0, 0.94939274892646675],\n",
       " [0, 1, 1, 200, 0, 0.95027217860617286],\n",
       " [0, 1, 1, 300, 0, 0.95432108710335029],\n",
       " [2, 0, 0, 50, 0, 0.84061061531235326],\n",
       " [2, 0, 0, 100, 0, 0.82614681670735235],\n",
       " [2, 0, 0, 200, 0, 0.80322918251642306],\n",
       " [2, 0, 0, 300, 0, 0.7818150865424266],\n",
       " [2, 0, 1, 50, 0, 0.93049583551095216],\n",
       " [2, 0, 1, 100, 0, 0.94467926708521044],\n",
       " [2, 0, 1, 200, 0, 0.94740266871307688],\n",
       " [2, 0, 1, 300, 0, 0.94947020466308096],\n",
       " [0, 1, 0, 50, 1, 0.87854366630933867],\n",
       " [0, 1, 0, 100, 1, 0.85900435609698678],\n",
       " [0, 1, 0, 200, 1, 0.82846319005099667],\n",
       " [0, 1, 0, 300, 1, 0.79783442184121012],\n",
       " [0, 1, 1, 50, 1, 0.91718013204612636],\n",
       " [0, 1, 1, 100, 1, 0.93777504445945359],\n",
       " [0, 1, 1, 200, 1, 0.94182364313465483],\n",
       " [0, 1, 1, 300, 1, 0.940943516355502],\n",
       " [2, 0, 0, 50, 1, 0.85254026113783055],\n",
       " [2, 0, 0, 100, 1, 0.83892241502876674],\n",
       " [2, 0, 0, 200, 1, 0.82577228834097449],\n",
       " [2, 0, 0, 300, 1, 0.80924179175570088],\n",
       " [2, 0, 1, 50, 1, 0.92542475142068947],\n",
       " [2, 0, 1, 100, 1, 0.93669597355543943],\n",
       " [2, 0, 1, 200, 1, 0.93697722265958361],\n",
       " [2, 0, 1, 300, 1, 0.93650729805480781]]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_to_pandas"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
