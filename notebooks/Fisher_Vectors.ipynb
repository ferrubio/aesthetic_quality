{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import os, os.path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import sys\n",
    "import pickle\n",
    "import gzip\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "mainPath='/home/frubio/AVA/'\n",
    "featuresPath = \"/home/frubio/aesthetic_quality/features/dSIFT/initialRad{:d}_scales{:d}_factor{:.1f}/AVA/\"\n",
    "\n",
    "sys.path.insert(0,'../pycode')\n",
    "import fisher_vector\n",
    "from sklearn import svm\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Parameters of the classification\n",
    "delta = 0\n",
    "\n",
    "# Parameters of the descriptors\n",
    "scales = 5\n",
    "initial_radius = 16\n",
    "factor_step = 1.2\n",
    "\n",
    "# Parameters for the FV\n",
    "size_patch = 1000000\n",
    "size_PCA = 64\n",
    "size_gmm = 256\n",
    "\n",
    "# Parameters for the cross validation\n",
    "np.random.seed(1000)\n",
    "batches = 100\n",
    "num_folds = 5\n",
    "folds = np.random.choice(range(0,batches),replace=False,size=(num_folds,int(batches/num_folds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "data = pickle.load(gzip.open('../packages/info.pklz','rb',2))\n",
    "num_images = len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "data.loc[:,'id'] = data['id'].apply(str)\n",
    "classes = np.array(data.sort_values(['id']).loc[:,'Class'])\n",
    "means = np.array(data.sort_values(['id']).loc[:,'VotesMean'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "indexes = np.array(range(0,len(classes))[:len(classes)-(len(classes) % batches)])\n",
    "indexes = indexes.reshape((batches,-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def balance_class(indexes, classes):\n",
    "    classes_uniques = np.unique(classes)\n",
    "    min_class = np.array([0,float('Inf')])\n",
    "    for i in classes_uniques:\n",
    "        aux_value = np.sum(classes == i)\n",
    "        if aux_value < min_class[1]:\n",
    "            min_class = np.array([i,aux_value])\n",
    "            \n",
    "    final_indexes = np.where(classes == min_class[0])[0]\n",
    "    for i in classes_uniques:\n",
    "        if i != min_class[0]:\n",
    "            aux_indexes = np.where(classes == i)[0]\n",
    "            #print np.random.choice(aux_indexes,replace=False,size=min_class[1])\n",
    "            final_indexes = np.concatenate((final_indexes,np.random.choice(aux_indexes,replace=False,size=min_class[1])))\n",
    "            \n",
    "    final_indexes = np.sort(final_indexes)\n",
    "    \n",
    "    return (final_indexes,classes[final_indexes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class FV_dictionary:\n",
    "    \n",
    "    def __init__(self,size_PCA, size_patch, size_gmm):\n",
    "        self.size_patch = size_patch\n",
    "        self.pca = PCA(n_components=size_PCA)\n",
    "        self.gmm = GaussianMixture(n_components=size_gmm, covariance_type='diag')\n",
    "        self.size_descriptor = 0\n",
    "        \n",
    "    def generate_dict(self,indexes,files,path):\n",
    "        \n",
    "        matrix_features = self.extract_patch_features(indexes, files, path)\n",
    "        \n",
    "        descriptor_size = matrix_features.shape[1]\n",
    "        if descriptor_size > self.pca.n_components:\n",
    "            self.pca.fit(matrix_features)\n",
    "            matrix_features = self.pca.transform(matrix_features)\n",
    "            self.size_descriptor = self.pca.n_components\n",
    "        else:\n",
    "            self.size_descriptor = descriptor_size\n",
    "            \n",
    "        self.gmm.fit(matrix_features)\n",
    "        \n",
    "    def obtain_fv(self,indexes,files,path):\n",
    "        \n",
    "        fv_size = self.gmm.n_components*(1+2*self.size_descriptor)\n",
    "        final_matrix = np.zeros((indexes.shape[0],fv_size))\n",
    "        counter = 0\n",
    "        for i in indexes:\n",
    "            fname=path+files[i]+'.pklz'\n",
    "            if os.path.isfile(fname):\n",
    "                sift = pickle.load(gzip.open(fname,\"rb\",2))\n",
    "                descriptor_size = sift.shape[1]\n",
    "                if descriptor_size > self.pca.n_components:\n",
    "                    sift = self.pca.transform(sift)\n",
    "                final_matrix[counter] = fisher_vector.fisher_vector(sift, self.gmm)\n",
    "                counter += 1\n",
    "        return final_matrix\n",
    "    \n",
    "    def extract_patch_features(self, indexes, files, path):\n",
    "        # We extract the number of vectors corresponding to the size of the patch / number of images\n",
    "        nImages = indexes.shape[0]\n",
    "        featuresPerImage = int(self.size_patch / nImages)\n",
    "        finalMatrix = np.zeros((featuresPerImage*nImages, 128),dtype=np.float32)\n",
    "\n",
    "        counter = 0\n",
    "        \n",
    "        for i in indexes:\n",
    "            fname=path+files[i]+'.pklz'\n",
    "            if os.path.isfile(fname):\n",
    "                sift = pickle.load(gzip.open(fname,\"rb\",2))\n",
    "                selectedFeat = np.random.choice(range(0,sift.shape[0]),replace=False,size=featuresPerImage)\n",
    "                finalMatrix[counter:counter+featuresPerImage] = sift[selectedFeat]\n",
    "            counter += featuresPerImage\n",
    "        return finalMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "sum_folds_sgd = 0\n",
    "sum_folds_nbg = 0\n",
    "matrix_sgd = np.zeros((2,2))\n",
    "matrix_nbg = np.zeros((2,2))\n",
    "for i in range(0, 1):\n",
    "    \n",
    "    # Prepare train\n",
    "    train_indexes = indexes[np.delete(folds,i,axis=0).reshape(-1)].reshape(-1)\n",
    "    train_means = means[train_indexes]\n",
    "    \n",
    "    # Delete values depending on the delta\n",
    "    vector_out_delta = (train_means <= 5-delta) | (train_means >= 5+delta)\n",
    "    train_indexes = train_indexes[vector_out_delta]\n",
    "    train_classes = classes[train_indexes]\n",
    "    \n",
    "    # Class balance\n",
    "    train_indexes,train_classes = balance_class(train_indexes,train_classes)\n",
    "    \n",
    "    # Only take into account those features from the final train set\n",
    "    dictionary = FV_dictionary(size_PCA,size_patch,size_gmm)\n",
    "    dictionary.generate_dict(train_indexes,np.array(data.sort_values(['id'])['id']),featuresPath.format(initial_radius,scales,factor_step))\n",
    "    train_features = dictionary.obtain_fv(train_indexes,np.array(data.sort_values(['id'])['id']),featuresPath.format(initial_radius,scales,factor_step))\n",
    "    \n",
    "    \n",
    "    # Fit models\n",
    "    sgd_clf = SGDClassifier(loss=\"hinge\", penalty=\"l2\")\n",
    "    sgd_clf.fit(train_features, train_classes)\n",
    "\n",
    "    nbg_clf = GaussianNB()\n",
    "    nbg_clf.fit(train_features, train_classes)\n",
    "    \n",
    "    # Prepare test\n",
    "    test_indices = indexes[folds[i]].reshape(-1)\n",
    "    test_features = dictionary.obtain_fv(test_indexes,np.array(data.sort_values(['id'])['id']),featuresPath.format(initial_radius,scales,factor_step))\n",
    "    test_classes = classes[test_indices]\n",
    "    \n",
    "    # Evaluate SVM model\n",
    "    predictions = sgd_clf.predict(test_features)\n",
    "    results = np.sum(predictions == test_classes)/float(len(predictions))\n",
    "    sum_folds_sgd += results\n",
    "    \n",
    "    matrix_sgd[0,0] += np.sum(predictions[predictions == test_classes] == 0)\n",
    "    matrix_sgd[0,1] += np.sum(predictions[predictions != test_classes] == 1)\n",
    "    matrix_sgd[1,0] += np.sum(predictions[predictions != test_classes] == 0)\n",
    "    matrix_sgd[1,1] += np.sum(predictions[predictions == test_classes] == 1)\n",
    "    \n",
    "    # Evaluate gnb model\n",
    "    predictions = nbg_clf.predict(test_features)\n",
    "    results = np.sum(predictions == test_classes)/float(len(predictions))\n",
    "    sum_folds_nbg += results\n",
    "    \n",
    "    matrix_nbg[0,0] += np.sum(predictions[predictions == test_classes] == 0)\n",
    "    matrix_nbg[0,1] += np.sum(predictions[predictions != test_classes] == 1)\n",
    "    matrix_nbg[1,0] += np.sum(predictions[predictions != test_classes] == 0)\n",
    "    matrix_nbg[1,1] += np.sum(predictions[predictions == test_classes] == 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "data_results = {'accuracy':sum_folds_svm/num_folds, 'conf_matrix':matrix_svm, 'classifier':'SGD', 'descriptor':total_files[int(sys.argv[1])], 'delta':delta}\n",
    "pickle.dump(data_results, gzip.open(\"../results/SGD_balanced_DescriptorFV_delta%f.pklz\" % (delta), \"wb\" ), 2)\n",
    "\n",
    "data_results = {'accuracy':sum_folds_nbg/num_folds, 'conf_matrix':matrix_nbg, 'classifier':'NB-G', 'descriptor':total_files[int(sys.argv[1])], 'delta':delta}\n",
    "pickle.dump(data_results, gzip.open(\"../results/GNB_balanced_DescriptorFV_delta%d.pklz\" % (delta), \"wb\" ), 2)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
