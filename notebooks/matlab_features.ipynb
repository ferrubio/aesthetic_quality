{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifiers based on features extracted from matlab\n",
    "\n",
    "In this notebook we use the different arff files obtained from matlab. We will use this features to obtain classifiers and test them in a cross validation process.\n",
    "\n",
    "## A bit of set up\n",
    "\n",
    "We need numpy and pandas for data. Pickle and gzip for read the extracted features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set up Python environment: numpy for numerical routines\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# for store the results\n",
    "import pickle\n",
    "import gzip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example we only use the default linear SVM classifier from libsvm and the Gaussian NB from sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/frubio/python/mlframework/mlframework/')\n",
    "sys.path.append('pycode/')\n",
    "from dataset.normal_dataset import normal_dataset\n",
    "import utilsData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature reading\n",
    "\n",
    "In this example only one package is read, but each ones have a size of 80Mb approximately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data = pickle.load(gzip.open('packages/info.pklz','rb',2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "total_files=['PHOG/','CHIST.arff', 'GHIST.arff', 'GIST_ori8_block4.arff', 'Centrist.arff']\n",
    "phog_files=['2_bins360_levels0_angle360.arff', '3_bins300_levels0_angle360.arff', '4_bins200_levels0_angle360.arff',\n",
    "            '5_bins100_levels0_angle360.arff', '6_bins50_levels0_angle360.arff', '7_bins20_levels0_angle360.arff',\n",
    "            '8_bins100_levels1_angle360.arff', '9_bins50_levels1_angle360.arff', '10_bins20_levels1_angle360_redux.arff',\n",
    "            '11_bins50_levels2_angle360.arff', '12_bins20_levels2_angle360.arff']\n",
    "\n",
    "delta = int(sys.argv[3])\n",
    "# this is for use the same random than in DeCAF\n",
    "batches = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if sys.argv[1] == '0':\n",
    "    features = utilsData.readARFF('features/AVA/'+total_files[int(sys.argv[1])]+phog_files[int(sys.argv[2])])\n",
    "else:\n",
    "    features = utilsData.readARFF('features/AVA/'+total_files[int(sys.argv[1])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This block must be eliminated, is only for test in the notebook\n",
    "batches = 10\n",
    "delta = 0.5\n",
    "sys.path.append('/home/frubio/PycharmProjects/mlframework/mlframework/')\n",
    "sys.path.append('../pycode/')\n",
    "from dataset.normal_dataset import normal_dataset\n",
    "import utilsData\n",
    "features = utilsData.readARFF('../features/AVA/PHOG/10_bins20_levels1_angle360_redux.arff')\n",
    "data = pickle.load(gzip.open('../packages/info.pklz','rb',2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "features=pd.DataFrame(features['data'],columns=features['vars'])\n",
    "features=features.rename(columns=(lambda x: 'var'+str(int(x[3:])-1)))\n",
    "features=features.rename(columns={'var0':'id'})\n",
    "\n",
    "num_features = len(features.columns)-1\n",
    "data=pd.merge(data, features, on='id', how='right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_images = len(data)\n",
    "\n",
    "votesList=np.array(data.iloc[:,2:12])\n",
    "auxTotal=np.sum(votesList,axis=1)\n",
    "auxMeanVector=np.sum(votesList*range(1,11),axis=1)/auxTotal.astype(np.float)\n",
    "auxClass=np.array(auxMeanVector >= 5, dtype=np.int)\n",
    "\n",
    "data.loc[:,'VotesMean'] = pd.Series(auxMeanVector, index=data.index)\n",
    "data.loc[:,'Class'] = pd.Series(auxClass, index=data.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data.loc[:,'id'] = data['id'].apply(str)\n",
    "classes = np.array(data.sort_values(['id']).loc[:,'Class'])\n",
    "features = np.array(data.sort_values(['id']).iloc[:,37:num_features+37])\n",
    "means = np.array(data.sort_values(['id']).loc[:,'VotesMean'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "indexes = np.array(range(0,len(classes))[:len(classes)-(len(classes) % batches)])\n",
    "indexes = indexes.reshape((batches,-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross validation\n",
    "\n",
    "In this case, we prepare vectors with the batches of each fold in order to test them in galgo and store the results.\n",
    "\n",
    "* First, we split the batches in 5 folds:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(1000)\n",
    "num_folds = 5\n",
    "folds = np.random.choice(range(0,batches),replace=False,size=(num_folds,batches/num_folds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def balance_class(features, classes):\n",
    "    classes_uniques = np.unique(classes)\n",
    "    min_class = np.array([0,float('Inf')])\n",
    "    for i in classes_uniques:\n",
    "        aux_value = np.sum(classes == i)\n",
    "        if aux_value < min_class[1]:\n",
    "            min_class = np.array([i,aux_value])\n",
    "            \n",
    "    final_indexes = np.where(classes == min_class[0])[0]\n",
    "    for i in classes_uniques:\n",
    "        if i != min_class[0]:\n",
    "            aux_indexes = np.where(classes == i)[0]\n",
    "            #print np.random.choice(aux_indexes,replace=False,size=min_class[1])\n",
    "            final_indexes = np.concatenate((final_indexes,np.random.choice(aux_indexes,replace=False,size=min_class[1])))\n",
    "            \n",
    "    final_indexes = np.sort(final_indexes)\n",
    "    \n",
    "    return (features[final_indexes],classes[final_indexes])\n",
    "                                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sum_folds_svm = 0\n",
    "sum_folds_nbg = 0\n",
    "matrix_svm = np.zeros((2,2))\n",
    "matrix_nbg = np.zeros((2,2))\n",
    "for i in range(0, num_folds):\n",
    "    \n",
    "    # Prepare train\n",
    "    train_indices = indexes[np.delete(folds,i,axis=0).reshape(-1)].reshape(-1)\n",
    "    train_features = features[train_indices]\n",
    "    train_classes = classes[train_indices]\n",
    "    train_means = means[train_indices]\n",
    "    \n",
    "    # Delete values depending on the delta\n",
    "    vector_out_delta = (train_means <= 5-delta) | (train_means >= 5+delta)\n",
    "    train_features = train_features[vector_out_delta]\n",
    "    train_classes = train_classes[vector_out_delta]\n",
    "    \n",
    "    # Class balance\n",
    "    train_features,train_classes = balance_class(train_features,train_classes)\n",
    "    \n",
    "    # Fit models\n",
    "    svm_clf = svm.LinearSVC()\n",
    "    svm_clf.fit(train_features, train_classes)\n",
    "\n",
    "    nbg_clf = GaussianNB()\n",
    "    nbg_clf.fit(train_features, train_classes)\n",
    "    \n",
    "    # Prepare test\n",
    "    test_indices = indexes[folds[i]].reshape(-1)\n",
    "    test_features = features[test_indices]\n",
    "    test_classes = classes[test_indices]\n",
    "    \n",
    "    # Evaluate SVM model\n",
    "    predictions = svm_clf.predict(test_features)\n",
    "    results = np.sum(predictions == test_classes)/float(len(predictions))\n",
    "    sum_folds_svm += results\n",
    "    \n",
    "    matrix_svm[0,0] += np.sum(predictions[predictions == test_classes] == 0)\n",
    "    matrix_svm[0,1] += np.sum(predictions[predictions != test_classes] == 1)\n",
    "    matrix_svm[1,0] += np.sum(predictions[predictions != test_classes] == 0)\n",
    "    matrix_svm[1,1] += np.sum(predictions[predictions == test_classes] == 1)\n",
    "    \n",
    "    # Evaluate gnb model\n",
    "    predictions = nbg_clf.predict(test_features)\n",
    "    results = np.sum(predictions == test_classes)/float(len(predictions))\n",
    "    sum_folds_nbg += results\n",
    "    \n",
    "    matrix_nbg[0,0] += np.sum(predictions[predictions == test_classes] == 0)\n",
    "    matrix_nbg[0,1] += np.sum(predictions[predictions != test_classes] == 1)\n",
    "    matrix_nbg[1,0] += np.sum(predictions[predictions != test_classes] == 0)\n",
    "    matrix_nbg[1,1] += np.sum(predictions[predictions == test_classes] == 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_results = {'accuracy':sum_folds_svm/num_folds, 'conf_matrix':matrix_svm, 'classifier':'SVM-L', 'descriptor':total_files[int(sys.argv[1])], 'delta':delta}\n",
    "if sys.argv[1] == '0':\n",
    "    data_results['case']=phog_files[int(sys.argv[2])]\n",
    "    pickle.dump(data_results, gzip.open(\"results/SVM_balanced_Descriptor%d_Case%d.pklz\" % (int(sys.argv[1]),int(sys.argv[2])), \"wb\" ), 2)\n",
    "else:\n",
    "    pickle.dump(data_results, gzip.open(\"results/SVM_balanced_Descriptor%d.pklz\" % (int(sys.argv[1])), \"wb\" ), 2)\n",
    "\n",
    "data_results = {'accuracy':sum_folds_nbg/num_folds, 'conf_matrix':matrix_nbg, 'classifier':'NB-G', 'descriptor':total_files[int(sys.argv[1])], 'delta':delta}\n",
    "if sys.argv[1] == '0':\n",
    "    data_results['case']=phog_files[int(sys.argv[2])]\n",
    "    pickle.dump(data_results, gzip.open(\"results/GNB_balanced_Descriptor%d_Case%d.pklz\" % (int(sys.argv[1]),int(sys.argv[2])), \"wb\" ), 2)\n",
    "else:\n",
    "    pickle.dump(data_results, gzip.open(\"results/GNB_balanced_Descriptor%d.pklz\" % (int(sys.argv[1])), \"wb\" ), 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dict_rest = pickle.load(gzip.open('../results/GNB_balanced_Descriptor0_Case0.pklz','rb',2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dict_rest"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
