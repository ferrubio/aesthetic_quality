{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A bit of set up\n",
    "\n",
    "We need numpy and pandas for data. Pickle and gzip for read the extracted features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set up Python environment: numpy for numerical routines\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# for store the results\n",
    "import pickle\n",
    "import gzip\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example we only use the default linear SVM classifier from libsvm and the Gaussian NB from sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/frubio/Mat_Docs/libsvm-3.20/python/')\n",
    "\n",
    "from svmutil import *\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature reading\n",
    "\n",
    "In this example only one package is read, but each ones have a size of 80Mb approximately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "select_layer = 0\n",
    "path_layers = (\"fc6_caffenet\",\"fc7_caffenet\",\"pool5_caffenet\",\"pool5_ResNet-152\")\n",
    "directory_file = \"%s/%s\"%(path_layers[select_layer],path_layers[select_layer])+\"_%02d.pklz\"\n",
    "batches = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "features=pickle.load(gzip.open(directory_file % 0,'rb',2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_H = features.shape[0]\n",
    "features = features.reshape((batch_H,-1))\n",
    "batch_W = features.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now is the turn for the classes:\n",
    "* First we read the information of AVA in pandas dataframe format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data=pickle.load(gzip.open('../info.pklz','rb',2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We calculate the mean of the votes and the weight (class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_images=len(data)\n",
    "auxWeight=np.zeros(num_images,dtype=np.int)\n",
    "auxMeanVector=np.zeros(num_images, dtype=np.double)\n",
    "votesList=np.array(data.iloc[:,2:12])\n",
    "auxTotal=np.sum(votesList,axis=1)\n",
    "auxMeanVector=np.sum(votesList*range(1,11),axis=1)/auxTotal.astype(np.float)\n",
    "auxWeight=np.array(auxMeanVector>=5, dtype=np.int)\n",
    "\n",
    "# for initial class 1 or 0\n",
    "data.loc[:,'Weight'] = pd.Series(auxWeight, index=data.index)\n",
    "data.loc[:,'VotesMean'] = pd.Series(auxMeanVector, index=data.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Finally, we transform the id to string and sort the information to extract the corresponding classes in a vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data.loc[:,'id'] = data['id'].apply(str)\n",
    "classes = np.array(data.sort_values(['id']).loc[:,'Weight'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* In order to have the same structure with respect to the features, where they are splitted in batches, we do the same with the classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "classes = classes[:len(classes)-(len(classes) % batches)]\n",
    "classes = classes.reshape((batches,-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross validation\n",
    "\n",
    "In this case, we prepare vectors with the batches of each fold in order to test them in galgo and store the results.\n",
    "\n",
    "* First, we split the batches in 5 folds:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.random.seed(1000)\n",
    "num_folds = 5\n",
    "folds = np.random.choice(range(0,batches),replace=False,size=(num_folds,batches/num_folds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We start the for, where the features are read and resimensioned in order to train the model, and then, the test is read in the same way and the predictions are made"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_and_format_features(indices_list,batch_H,batch_W,directory_file):\n",
    "    num_batches = len(indices_list)\n",
    "    features = np.zeros((num_batches*batch_H, batch_W))\n",
    "    \n",
    "    pre_count = 0\n",
    "    post_count = batch_H\n",
    "    \n",
    "    for i in indices_list:\n",
    "        features_aux = pickle.load(gzip.open(directory_file % i,'rb',2))\n",
    "        features[pre_count:post_count] = features_aux.reshape((batch_H, batch_W))\n",
    "        pre_count = post_count\n",
    "        post_count += batch_H \n",
    "        \n",
    "    return features\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def read_and_format_classes(indices_list, batch_H, classes):\n",
    "    num_batches = len(indices_list)\n",
    "    train_classes = np.zeros(num_batches*batch_H)\n",
    "    \n",
    "    pre_count = 0\n",
    "    post_count = batch_H\n",
    "\n",
    "    for i in indices_list:\n",
    "        train_classes[pre_count:post_count] = classes[i]\n",
    "        \n",
    "        pre_count = post_count\n",
    "        post_count += batch_H \n",
    "        \n",
    "    return train_classes\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def balance_class(features, classes):\n",
    "    classes_uniques = np.unique(classes)\n",
    "    min_class = np.array([0,float('Inf')])\n",
    "    for i in classes_uniques:\n",
    "        aux_value = np.sum(classes == i)\n",
    "        if aux_value < min_class[1]:\n",
    "            min_class = np.array([i,aux_value])\n",
    "            \n",
    "    final_indexes = np.where(classes == min_class[0])[0]\n",
    "    for i in classes_uniques:\n",
    "        if i != min_class[0]:\n",
    "            aux_indexes = np.where(classes == i)[0]\n",
    "            #print np.random.choice(aux_indexes,replace=False,size=min_class[1])\n",
    "            final_indexes = np.concatenate((final_indexes,np.random.choice(aux_indexes,replace=False,size=min_class[1])))\n",
    "            \n",
    "    final_indexes = np.sort(final_indexes)\n",
    "    \n",
    "    return (features[final_indexes],classes[final_indexes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results = np.zeros(num_folds)\n",
    "for i in range(0, num_folds):\n",
    "    \n",
    "    # Prepare train\n",
    "    train_indices = np.delete(folds,i,axis=0).reshape(-1)\n",
    "    \n",
    "    features = read_and_format_features(train_indices[0:1],batch_H,batch_W,directory_file)\n",
    "    train_classes = read_and_format_classes(train_indices[0:1],batch_H,classes)\n",
    "    features,train_classes = balance_class(features,train_classes)\n",
    "    \n",
    "    # PCA\n",
    "    pca = PCA(n_components=512)\n",
    "    pca.fit(features)\n",
    "    features = pca.transform(features)\n",
    "    \n",
    "    # Fit models\n",
    "    prob  = svm_problem(train_classes.tolist(), features.tolist())\n",
    "    param = svm_parameter('-t 0')\n",
    "    svm_clf = svm_train(prob, param)\n",
    "\n",
    "    \n",
    "    # Prepare test\n",
    "    test_indices = folds[i]\n",
    "    \n",
    "    features = read_and_format_features(test_indices[0:1],batch_H,batch_W,directory_file)\n",
    "    test_classes = read_and_format_classes(test_indices[0:1],batch_H,classes)\n",
    "    \n",
    "    # PCA\n",
    "    features = pca.transform(features)\n",
    "    \n",
    "    # Evaluate model\n",
    "    _, p_acc, _ = svm_predict(test_classes.tolist(), features.tolist(), svm_clf)\n",
    "    pickle.dump(p_acc[0]/100, gzip.open( \"results/PCA_SVM_balanced_%s_fold%d.pklz\" % (path_layers[select_layer],i), \"wb\" ), 2)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
